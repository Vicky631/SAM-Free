import os
import json
from typing import Optional, Union, Tuple

import cv2
import torch
import numpy as np
from PIL import Image
from tqdm import tqdm


class FSC147DatasetLoader:
    """
    FSC147æ•°æ®é›†åŠ è½½ç±»ï¼ˆæ”¯æŒï¼šæ— ç¼©æ”¾ / ç­‰æ¯”ä¾‹ç¼©æ”¾ / æ‹‰ä¼¸ï¼‰
    å…¼å®¹ä¸¤ç§ç‚¹æ ‡æ³¨æ ¼å¼ï¼š
    1. å¹³é“ºå¼ï¼š{"points": [[x1,y1], [x2,y2], ...]}
    2. åµŒå¥—å¼ï¼š{"annotations": [{"points": [x1,y1]}, ...]}
    """

    def __init__(self,
                 annotation_file: str,
                 image_root: str,
                 max_size: int = 1024,
                 fixed_size: Optional[Tuple[int, int]] = None,
                 scale_mode: str = 'ratio'):
        self.annotation_file = annotation_file
        self.image_root = image_root
        self.max_size = max_size
        self.fixed_size = fixed_size
        self.scale_mode = scale_mode

        if self.scale_mode == 'fixed_stretch' and self.fixed_size is None:
            raise ValueError("fixed_stretch å¿…é¡»æŒ‡å®š fixed_size")
        if self.scale_mode not in ['ratio', 'fixed_stretch', 'none']:
            raise ValueError("scale_mode å¿…é¡»æ˜¯ ratio/fixed_stretch/none")

        with open(annotation_file, 'r', encoding='utf-8') as f:
            self.annotations = json.load(f)

        self._preprocess_annotations()

    def _calculate_scale(self, orig_w: int, orig_h: int) -> Tuple[float, float]:
        if self.scale_mode == 'ratio':
            scale = self.max_size / max(orig_w, orig_h)
            return scale, scale
        elif self.scale_mode == 'fixed_stretch':
            target_w, target_h = self.fixed_size
            return target_w / orig_w, target_h / orig_h
        else:  # none
            return 1.0, 1.0

    def _parse_points(self, target: dict) -> torch.Tensor:
        """
        å…¼å®¹ä¸¤ç§ç‚¹æ ‡æ³¨æ ¼å¼çš„è§£æžå‡½æ•°
        :param target: å•å¼ å›¾ç‰‡çš„æ ‡æ³¨å­—å…¸
        :return: ç‚¹åæ ‡å¼ é‡ [N, 2]
        """
        # æ ¼å¼1ï¼šå¹³é“ºå¼ points ç›´æŽ¥å­˜åœ¨
        if 'points' in target and isinstance(target['points'], list):
            points = torch.tensor(target['points'], dtype=torch.float32)
        # æ ¼å¼2ï¼šåµŒå¥—å¼ annotations ä¸‹çš„ points
        elif 'annotations' in target and isinstance(target['annotations'], (dict, list)):
            # å¤„ç† annotations æ˜¯å­—å…¸çš„æƒ…å†µï¼ˆå¦‚ {0: {...}, 1: {...}}ï¼‰
            if isinstance(target['annotations'], dict):
                annotations = list(target['annotations'].values())
            else:  # list æƒ…å†µ
                annotations = target['annotations']

            # æå–æ¯ä¸ªannotationçš„pointså¹¶å±•å¹³
            points_list = []
            for ann in annotations:
                if 'points' in ann and isinstance(ann['points'], list):
                    # å…¼å®¹ points æ˜¯ [[x,y]] æˆ– [x,y] ä¸¤ç§å­æ ¼å¼
                    if len(ann['points']) > 0 and isinstance(ann['points'][0], list):
                        points_list.extend(ann['points'])
                    else:
                        points_list.append(ann['points'])

            points = torch.tensor(points_list, dtype=torch.float32)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç‚¹æ ‡æ³¨æ ¼å¼ï¼ç›®æ ‡å­—å…¸é”®ï¼š{list(target.keys())}")

        # ç¡®ä¿æ˜¯äºŒç»´å¼ é‡ [N, 2]
        if points.ndim == 1:
            points = points.unsqueeze(0)

        return points

    def _preprocess_annotations(self):
        self.annotation_cache = {}
        for fname in tqdm(self.annotations.keys(), desc="Preprocessing annotations"):
            target = self.annotations[fname]
            orig_w, orig_h = target['width'], target['height']
            scale_w, scale_h = self._calculate_scale(orig_w, orig_h)

            # å…¼å®¹ä¸¤ç§æ ¼å¼è§£æžç‚¹åæ ‡
            orig_points = self._parse_points(target)

            # è§£æžç¤ºä¾‹æ¡†ï¼ˆä¸¤ç§æ ¼å¼çš„boxå­—æ®µé€šå¸¸ä¸€è‡´ï¼‰
            if 'box_examples_coordinates' in target:
                orig_boxes = torch.tensor(target['box_examples_coordinates'], dtype=torch.float32)
            else:
                raise KeyError(f"æœªæ‰¾åˆ°ç¤ºä¾‹æ¡†æ ‡æ³¨ 'box_examples_coordinates' - {fname}")

            # ç¼©æ”¾åæ ‡
            scaled_points = orig_points.clone()
            scaled_points[:, 0] *= scale_w
            scaled_points[:, 1] *= scale_h

            scaled_boxes = orig_boxes.clone()
            scaled_boxes[:, [0, 2]] *= scale_w
            scaled_boxes[:, [1, 3]] *= scale_h

            self.annotation_cache[fname] = {
                'orig_size': (orig_w, orig_h),
                'scale_w': scale_w,
                'scale_h': scale_h,
                'orig_points': orig_points,
                'orig_boxes': orig_boxes,
                'scaled_points': scaled_points,
                'scaled_boxes': scaled_boxes
            }

    def get_image(self, fname: str, return_scaled: bool = False) -> Union[Image.Image, Tuple[Image.Image, dict]]:
        img_path = os.path.join(self.image_root, fname)
        img = Image.open(img_path).convert("RGB")

        if not return_scaled:
            return img

        orig_w, orig_h = img.size
        scale_w, scale_h = self._calculate_scale(orig_w, orig_h)

        if self.scale_mode == 'ratio':
            new_w = int(orig_w * scale_w)
            new_h = int(orig_h * scale_h)
            img_scaled = img.resize((new_w, new_h), Image.BILINEAR)
            img_padded = Image.new("RGB", (self.max_size, self.max_size), (255, 255, 255))
            img_padded.paste(img_scaled, (0, 0))
            img_scaled = img_padded
        elif self.scale_mode == 'fixed_stretch':
            img_scaled = img.resize(self.fixed_size, Image.BILINEAR)
        else:
            img_scaled = img.copy()

        scale_info = {
            'scale_w': scale_w,
            'scale_h': scale_h,
            'orig_size': (orig_w, orig_h),
            'scaled_size': img_scaled.size
        }
        return img_scaled, scale_info

    def get_annotations(self, fname: str, return_scaled: bool = False, return_numpy: bool = False):
        if fname not in self.annotation_cache:
            raise KeyError(f"æœªæ‰¾åˆ° {fname}")

        cache = self.annotation_cache[fname]
        result = {
            'orig_size': cache['orig_size'],
            'scale_w': cache['scale_w'],
            'scale_h': cache['scale_h']
        }

        if return_scaled:
            points = cache['scaled_points']
            boxes = cache['scaled_boxes']
        else:
            points = cache['orig_points']
            boxes = cache['orig_boxes']

        if return_numpy:
            points = points.numpy()
            boxes = boxes.numpy()

        result['points'] = points
        result['boxes'] = boxes
        return result

    def get_all_filenames(self):
        return list(self.annotations.keys())

    def __len__(self):
        return len(self.annotations)

    def __getitem__(self, idx: int):
        fname = self.get_all_filenames()[idx]
        img = self.get_image(fname)
        anns = self.get_annotations(fname)
        return {
            'filename': fname,
            'image': img,
            'orig_size': anns['orig_size'],
            'scale_w': anns['scale_w'],
            'scale_h': anns['scale_h'],
            'points': anns['points'],
            'boxes': anns['boxes']
        }


# -------------------------- å¯è§†åŒ–å‡½æ•° --------------------------
def visualize_annotations(img, points, boxes, save_path):
    img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)

    if isinstance(points, torch.Tensor):
        points = points.numpy()
    if isinstance(boxes, torch.Tensor):
        boxes = boxes.numpy()

    # å…¼å®¹å¯èƒ½çš„ç»´åº¦é—®é¢˜ï¼ˆç¡®ä¿pointsæ˜¯[N,2]ï¼‰
    if points.ndim == 1:
        points = points.reshape(-1, 2)

    # ç»˜åˆ¶ç‚¹æ ‡æ³¨
    for (x, y) in points:
        cv2.circle(img_cv, (int(x), int(y)), radius=5, color=(0, 0, 255), thickness=-1)

    # ç»˜åˆ¶ç¤ºä¾‹æ¡†ï¼ˆå…¼å®¹boxeså¯èƒ½çš„ç»´åº¦ï¼‰
    if boxes.ndim == 2 and boxes.shape[1] == 4:
        for (x1, y1, x2, y2) in boxes:
            cv2.rectangle(img_cv, (int(x1), int(y1)), (int(x2), int(y2)), color=(0, 255, 0), thickness=2)
    # å…¼å®¹boxæ˜¯åµŒå¥—æ ¼å¼ [[x1,y1],[x2,y2]] çš„æƒ…å†µ
    elif boxes.ndim == 3 and boxes.shape[1:] == (4, 2):
        for box in boxes:
            x1, y1 = box[0][0], box[0][1]
            x2, y2 = box[2][0], box[2][1]
            cv2.rectangle(img_cv, (int(x1), int(y1)), (int(x2), int(y2)), color=(0, 255, 0), thickness=2)

    cv2.imwrite(save_path, img_cv)
    print(f"å¯è§†åŒ–å·²ä¿å­˜: {save_path}")


# -------------------------- å®Œæ•´æµ‹è¯•ç”¨ä¾‹ --------------------------
if __name__ == "__main__":
    annotation_file = '/data/wjj/dataset/FSC_147/annotation_FSC147_384_with_gt.json'
    image_root = '/data/wjj/dataset/FSC_147/images_384_VarV2'

    # æµ‹è¯•å›¾ç‰‡åˆ—è¡¨ï¼ˆé€‰å‡ å¼ ä¸åŒé•¿å®½æ¯”çš„å›¾ï¼‰
    test_filenames = [
        '2.jpg'
    ]

    # --------------------------------------------------
    # æµ‹è¯• 1ï¼šæ— ç¼©æ”¾æ¨¡å¼ï¼ˆåŽŸå›¾å°ºå¯¸ï¼‰
    # --------------------------------------------------
    print("\n=== æµ‹è¯• 1ï¼šæ— ç¼©æ”¾æ¨¡å¼ ===")
    loader_none = FSC147DatasetLoader(
        annotation_file=annotation_file,
        image_root=image_root,
        scale_mode='none'
    )

    for fname in test_filenames:
        img, scale_info = loader_none.get_image(fname, return_scaled=True)
        anns = loader_none.get_annotations(fname, return_scaled=True)

        assert scale_info['scale_w'] == 1.0
        assert scale_info['scale_h'] == 1.0
        assert img.size == anns['orig_size']

        print(f"æ— ç¼©æ”¾æ¨¡å¼ - {fname}: å°ºå¯¸={img.size}, scale={scale_info['scale_w']:.4f}")

        visualize_annotations(
            img=img,
            points=anns['points'],
            boxes=anns['boxes'],
            save_path=f'./test_none_{fname}'
        )

    # --------------------------------------------------
    # æµ‹è¯• 2ï¼šç­‰æ¯”ä¾‹ç¼©æ”¾æ¨¡å¼ï¼ˆå¸¦ç™½è¾¹ï¼‰
    # --------------------------------------------------
    print("\n=== æµ‹è¯• 2ï¼šç­‰æ¯”ä¾‹ç¼©æ”¾æ¨¡å¼ ===")
    loader_ratio = FSC147DatasetLoader(
        annotation_file=annotation_file,
        image_root=image_root,
        max_size=512,
        scale_mode='ratio'
    )

    for fname in test_filenames:
        img, scale_info = loader_ratio.get_image(fname, return_scaled=True)
        anns = loader_ratio.get_annotations(fname, return_scaled=True)

        print(f"ç­‰æ¯”ä¾‹ç¼©æ”¾ - {fname}: åŽŸå›¾å°ºå¯¸={anns['orig_size']}, ç¼©æ”¾åŽ={img.size}, scale={scale_info['scale_w']:.4f}")

        visualize_annotations(
            img=img,
            points=anns['points'],
            boxes=anns['boxes'],
            save_path=f'./test_ratio_{fname}'
        )

    # --------------------------------------------------
    # æµ‹è¯• 3ï¼šæ‹‰ä¼¸æ¨¡å¼ï¼ˆæ— ç™½è¾¹ï¼‰
    # --------------------------------------------------
    print("\n=== æµ‹è¯• 3ï¼šæ‹‰ä¼¸æ¨¡å¼ ===")
    loader_stretch = FSC147DatasetLoader(
        annotation_file=annotation_file,
        image_root=image_root,
        fixed_size=(512, 512),
        scale_mode='fixed_stretch'
    )

    for fname in test_filenames:
        img, scale_info = loader_stretch.get_image(fname, return_scaled=True)
        anns = loader_stretch.get_annotations(fname, return_scaled=True)

        assert img.size == (512, 512)

        print(f"æ‹‰ä¼¸æ¨¡å¼ - {fname}: ç›®æ ‡å°ºå¯¸=(512,512), å®žé™…å°ºå¯¸={img.size}")

        visualize_annotations(
            img=img,
            points=anns['points'],
            boxes=anns['boxes'],
            save_path=f'./test_stretch_{fname}'
        )

    # --------------------------------------------------
    # æµ‹è¯• 4ï¼šæ‰¹é‡éšæœºæµ‹è¯•ï¼ˆéªŒè¯ 100 å¼ å›¾ä¸æŠ¥é”™ï¼‰
    # --------------------------------------------------
    print("\n=== æµ‹è¯• 4ï¼šæ‰¹é‡éšæœºæµ‹è¯• 100 å¼ å›¾ ===")
    loader = FSC147DatasetLoader(
        annotation_file=annotation_file,
        image_root=image_root,
        scale_mode='none'
    )

    all_filenames = loader.get_all_filenames()
    import random
    random.shuffle(all_filenames)

    for fname in tqdm(all_filenames[:100], desc="æ‰¹é‡æµ‹è¯•"):
        img, scale_info = loader.get_image(fname, return_scaled=True)
        anns = loader.get_annotations(fname, return_scaled=True)

        assert img.size == anns['orig_size']
        assert scale_info['scale_w'] == 1.0
        assert scale_info['scale_h'] == 1.0

    print("æ‰¹é‡æµ‹è¯•é€šè¿‡ï¼")

    # --------------------------------------------------
    # æµ‹è¯• 5ï¼šéªŒè¯åæ ‡æ˜¯å¦æ­£ç¡®ï¼ˆç¼©æ”¾åŽåæ ‡åº”åœ¨å›¾åƒèŒƒå›´å†…ï¼‰
    # --------------------------------------------------
    print("\n=== æµ‹è¯• 5ï¼šéªŒè¯åæ ‡æœ‰æ•ˆæ€§ ===")
    loader = FSC147DatasetLoader(
        annotation_file=annotation_file,
        image_root=image_root,
        fixed_size=(600, 400),
        scale_mode='fixed_stretch'
    )

    for fname in test_filenames:
        img, scale_info = loader.get_image(fname, return_scaled=True)
        anns = loader.get_annotations(fname, return_scaled=True)
        points = anns['points'].numpy()

        w, h = img.size
        assert (points[:, 0] >= 0).all()
        assert (points[:, 0] <= w).all()
        assert (points[:, 1] >= 0).all()
        assert (points[:, 1] <= h).all()

        print(f"åæ ‡éªŒè¯é€šè¿‡ - {fname}")

    print("\næ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ðŸŽ‰")